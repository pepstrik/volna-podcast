#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
RSS –ø–∞—Ä—Å–µ—Ä –¥–ª—è –ø–æ–¥–∫–∞—Å—Ç–∞ ¬´–í–æ–ª–Ω–∞ —Å –í–æ—Å—Ç–æ–∫–∞¬ª
–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç episodes.json –¥–ª—è –ø–æ–∏—Å–∫–∞ –Ω–∞ —Å–∞–π—Ç–µ
"""

import argparse
import feedparser
import json
import os
import re
import sys
from datetime import datetime, timezone
from html import unescape
from email.utils import parsedate_to_datetime

DEFAULT_RSS = os.getenv("RSS_URL")
# –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ RSS_URL —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω
if not DEFAULT_RSS:
    print("‚ùå –û–®–ò–ë–ö–ê: –ü–µ—Ä–µ–º–µ–Ω–Ω–∞—è –æ–∫—Ä—É–∂–µ–Ω–∏—è RSS_URL –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞!")
    print("   –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ –µ—ë –≤ GitHub Secrets –∏–ª–∏ –≤ .env —Ñ–∞–π–ª–µ")
    sys.exit(1)

DEFAULT_OUT = "episodes.json"
EXTRAS_FILE = os.getenv("EXTRAS_FILE", "extras_map.json")

def clean_html(text: str) -> str:
    """–ì—Ä—É–±–æ–µ —É–¥–∞–ª–µ–Ω–∏–µ HTML-—Ç–µ–≥–æ–≤ + unescape, —Å—Ö–ª–æ–ø—ã–≤–∞–Ω–∏–µ –ø—Ä–æ–±–µ–ª–æ–≤."""
    if not text:
        return ""
    text = unescape(text)
    text = re.sub(r"<[^>]+>", " ", text)
    text = re.sub(r"\s+", " ", text)
    # –ï—Å–ª–∏ —Å—Å—ã–ª–∫–∏ –≤ –æ–ø–∏—Å–∞–Ω–∏–∏ –Ω–µ –Ω—É–∂–Ω—ã –¥–ª—è —Å–∞–π—Ç–∞ ‚Äì —É–±–∏—Ä–∞–µ–º:
    text = re.sub(r"https?://\S+", "", text)
    return text.strip()


def parse_duration(raw) -> str:
    """–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∫ H:MM:SS –∏–ª–∏ M:SS."""
    if not raw:
        return ""
    s = str(raw).strip()

    # –ï—Å–ª–∏ —É–∂–µ —Ñ–æ—Ä–º–∞—Ç–∞ H:MM:SS –∏–ª–∏ M:SS ‚Äî –æ—Å—Ç–∞–≤–ª—è–µ–º
    if re.fullmatch(r"\d{1,2}:\d{2}(:\d{2})?", s):
        # –ü—Ä–∏–≤–µ–¥—ë–º –∫ H:MM:SS –ø—Ä–∏ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏
        parts = list(map(int, s.split(":")))
        if len(parts) == 2:
            m, sec = parts
            return f"{m}:{sec:02d}"
        h, m, sec = (parts + [0, 0])[:3]
        return f"{h}:{m:02d}:{sec:02d}"

    # –í–∞—Ä–∏–∞–Ω—Ç—ã "3723", "1h02m03s", "95m12s"
    m = re.fullmatch(r"(?:(\d+)h)?(?:(\d+)m)?(?:(\d+)s)?", s.lower())
    if m and any(m.groups()):
        h = int(m.group(1) or 0)
        mi = int(m.group(2) or 0)
        sec = int(m.group(3) or 0)
        if h > 0:
            return f"{h}:{mi:02d}:{sec:02d}"
        return f"{mi}:{sec:02d}"

    try:
        total = int(s)
        h = total // 3600
        mi = (total % 3600) // 60
        sec = total % 60
        if h > 0:
            return f"{h}:{mi:02d}:{sec:02d}"
        return f"{mi}:{sec:02d}"
    except (ValueError, TypeError):
        return s
        
def load_extras_map(path: str) -> dict:
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –∫–∞—Ä—Ç—É –¥–æ–ø.–ø–æ–ª–µ–π –ø–æ –Ω–æ–º–µ—Ä—É —ç–ø–∏–∑–æ–¥–∞."""
    if os.path.exists(path):
        try:
            with open(path, "r", encoding="utf-8") as f:
                data = json.load(f)
                if isinstance(data, dict):
                    return data
        except Exception as e:
            print(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ—á–∏—Ç–∞—Ç—å {path}: {e}")
    return {}

def norm_epnum(v) -> str:
    """–ù–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç episode_number –∫ —Å—Ç—Ä–æ–∫–µ ('8', '12'...), —á—Ç–æ–±—ã –∫–ª—é—á–∏ —Å–æ–≤–ø–∞–¥–∞–ª–∏ —Å JSON-–∫–∞—Ä—Ç–æ–π."""
    if v is None or v == "":
        return ""
    try:
        return str(int(v))
    except Exception:
        # –µ—Å–ª–∏ –≤–¥—Ä—É–≥ –≤ RSS —Å—Ç—Ä–æ–∫–∞, —Å–æ—Ö—Ä–∞–Ω—è–µ–º –∫–∞–∫ –µ—Å—Ç—å (–Ω–∞–ø—Ä–∏–º–µ—Ä, 'S1E8')
        return str(v).strip()


def coerce_datetime(entry) -> tuple[datetime | None, str, int | None]:
    """–î–æ—Å—Ç–∞—ë–º –¥–∞—Ç—É: published/updated (parsed -> datetime), —Å—Ç—Ä–æ–∫—É –¥–ª—è –≤—ã–≤–æ–¥–∞ –∏ –≥–æ–¥."""
    dt = None

    # Parsed –ø–æ–ª—è
    for k in ("published_parsed", "updated_parsed", "created_parsed"):
        if getattr(entry, k, None):
            try:
                dt = datetime(*getattr(entry, k)[:6], tzinfo=timezone.utc)
                break
            except Exception:
                pass

    # –°—Ç—Ä–æ–∫–æ–≤—ã–µ –ø–æ–ª—è
    if dt is None:
        for k in ("published", "updated", "created"):
            v = entry.get(k)
            if v:
                try:
                    dt = parsedate_to_datetime(v)
                    # –ü—Ä–∏–≤–µ–¥—ë–º naive –∫ UTC
                    if dt and dt.tzinfo is None:
                        dt = dt.replace(tzinfo=timezone.utc)
                    break
                except Exception:
                    continue

    # –§–æ—Ä–º–∞—Ç –¥–ª—è –≤—ã–≤–æ–¥–∞
    date_str = dt.astimezone(timezone.utc).strftime("%d.%m.%Y") if dt else ""
    year = dt.year if dt else None
    return dt, date_str, year


def pick_audio(entry) -> str:
    """–ò—â–µ–º –∞—É–¥–∏–æ —Å–Ω–∞—á–∞–ª–∞ –≤ enclosures, –ø–æ—Ç–æ–º –≤ links[rel=enclosure]."""
    # enclosures
    for enc in entry.get("enclosures", []):
        t = (enc.get("type") or "").lower()
        if "audio" in t or t in {"audio/mpeg", "audio/mp3", "audio/aac"}:
            return enc.get("href") or ""

    # links rel=enclosure
    for ln in entry.get("links", []):
        if ln.get("rel") == "enclosure":
            t = (ln.get("type") or "").lower()
            if not t or "audio" in t or t in {"audio/mpeg", "audio/mp3", "audio/aac"}:
                return ln.get("href") or ""

    return ""


def pick_image(entry) -> str:
    """itunes:image / media:thumbnail / media:content ‚Äî –ø–µ—Ä–≤–∞—è –ø–æ–¥—Ö–æ–¥—è—â–∞—è."""
    itunes_image = entry.get("itunes_image")
    if isinstance(itunes_image, dict) and itunes_image.get("href"):
        return itunes_image["href"]

    # feedparser –º–æ–∂–µ—Ç —Ä–∞—Å–∫–ª–∞–¥—ã–≤–∞—Ç—å media_* –ø–æ-—Ä–∞–∑–Ω–æ–º—É
    for k in ("media_thumbnail", "media_content"):
        arr = entry.get(k) or []
        if isinstance(arr, list) and arr:
            href = arr[0].get("url") or arr[0].get("href")
            if href:
                return href
    return ""


def to_int_or_str(v):
    try:
        return int(v)
    except Exception:
        return str(v) if v is not None else ""


def parse_rss_to_json(rss_url: str, out_path: str) -> int:
    print(f"–ó–∞–≥—Ä—É–∂–∞—é RSS: {rss_url}")
    feed = feedparser.parse(rss_url)
    extras_map = load_extras_map(EXTRAS_FILE)

    if getattr(feed, "bozo", False):
        print(f"‚ö†Ô∏è –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ: {getattr(feed, 'bozo_exception', 'unknown parse issue')}")

    episodes = []
    for idx, entry in enumerate(feed.entries or [], 1):
        try:
            title = entry.get("title", "–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è").strip()
            # –æ–ø–∏—Å–∞–Ω–∏–µ: content -> summary_detail -> summary/description
            description = (
                (entry.get("content") or [{}])[0].get("value")
                or (entry.get("summary_detail") or {}).get("value")
                or entry.get("summary")
                or entry.get("description")
                or ""
            )
            description = clean_html(description)
            link = entry.get("link", "").strip()

            pub_dt, date_str, year = coerce_datetime(entry)
            audio_url = pick_audio(entry)
            image_url = pick_image(entry)

            duration = parse_duration(
                entry.get("itunes_duration")
                or entry.get("itunes:duration")
                or entry.get("duration")
            )
            episode_number = to_int_or_str(entry.get("itunes_episode") or entry.get("episode"))
            season = to_int_or_str(entry.get("itunes_season") or entry.get("season"))
            episode_type = entry.get("itunes_episodetype") or entry.get("episodeType") or ""

            guid = entry.get("guid") or entry.get("id") or ""
            explicit = str(entry.get("itunes_explicit") or "").lower() in {"yes", "true", "1"}

            num_key = norm_epnum(episode_number)
            extra = extras_map.get(num_key, {})  # ‚Üê –≤–æ—Ç –µ—ë –∏ –Ω–µ —Ö–≤–∞—Ç–∞–ª–æ

            episodes.append(
                {
                    "name": title,
                    "desc": description,
                    "link": link,
                    "audio_url": audio_url,
                    "image": image_url,
                    "date": date_str,
                    "year": year,
                    "duration": duration,
                    "episode_number": episode_number,
                    "season": season,
                    "episode_type": episode_type,
                    "guid": guid,
                    "explicit": explicit,
                    "page": extra.get("page", ""),
                    # ¬´—Å—ã—Ä–∞—è¬ª –¥–∞—Ç–∞ –¥–ª—è —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∏/–æ—Ç–ª–∞–¥–∫–∏ (ISO, UTC)
                    "pub_iso": pub_dt.astimezone(timezone.utc).isoformat() if pub_dt else "",
                }
            )
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –≤ –∑–∞–ø–∏—Å–∏ {idx}: {e}")
            continue

    # —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ —Ä–µ–∞–ª—å–Ω–æ–π –¥–∞—Ç–µ, –∑–∞—Ç–µ–º –ø–æ –∏–º–µ–Ω–∏ –∫–∞–∫ —Å—Ç–∞–±–∏–ª—å–Ω—ã–π fallback
    episodes.sort(
        key=lambda x: (x["pub_iso"] or "", x["name"]),
        reverse=True,
    )

    with open(out_path, "w", encoding="utf-8") as f:
        json.dump(episodes, f, ensure_ascii=False, indent=2)

    print(f"‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ {len(episodes)} –≤—ã–ø—É—Å–∫–æ–≤ ‚Üí {out_path}")
    if episodes:
        latest = episodes[0]
        print(f"üéô {latest['name']}  üìÖ {latest['date']}")

    # –Ω—É–ª–µ–≤–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ ‚Äî —Å–∏–≥–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –æ—à–∏–±–∫–æ–π –≤–æ–∑–≤—Ä–∞—Ç–∞
    return len(episodes)


def main():
    ap = argparse.ArgumentParser(description="Parse podcast RSS to JSON")
    ap.add_argument("--rss", default=DEFAULT_RSS, help="RSS URL (env RSS_URL by default)")
    ap.add_argument("--out", default=DEFAULT_OUT, help="Output JSON file path")
    args = ap.parse_args()

    count = parse_rss_to_json(args.rss, args.out)
    sys.exit(0 if count > 0 else 1)


if __name__ == "__main__":
    main()
